{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installation tensorflow_text \n",
    "!pip install -q -U \"tensorflow-text==2.8.*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes qui peuvent aider:\n",
    "- English-to-French translation\n",
    "- Bilingual, parallel corpora provided by ACL WMT  14\n",
    "\n",
    "reduce the size of the corpus using the selection method by Axelrod et al.\n",
    "\n",
    "\n",
    "Preprocesing of the data:\n",
    "- tokenization\n",
    "- use a shortlist of 30,000 most frequent words in each langage to train the model\n",
    "- any word not included in the shortlist is mapped to a special token ([UNK])\n",
    "- no other preprocessing method (no lowercasinf or stemming)\n",
    "\n",
    "- 2 types of data \n",
    "\n",
    "\n",
    "sentences of up to 30 words and sentences of up to 50 words\n",
    "\n",
    "- each update direction is computed using a minibatch of 80 sentences\n",
    "\n",
    "- once the model is trained, use a beam search to find a translation that approximately maximizes the conditional probability\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "True\n",
      "Ma question porte sur un sujet qui est à l'ordre du jour du jeudi et que je soulèverai donc une nouvelle fois.\n",
      "\n",
      "My question relates to something that will come up on Thursday and which I will then raise again.\n",
      "tf.Tensor(\n",
      "[b\"Oui, Monsieur Evans, je pense qu'une initiative dans le sens que vous venez de sugg\\xc3\\xa9rer serait tout \\xc3\\xa0 fait appropri\\xc3\\xa9e.\\n\"\n",
      " b\"Je voudrais vous demander un conseil au sujet de l'article 143, qui concerne l'irrecevabilit\\xc3\\xa9.\\n\"\n",
      " b\"L'une des personnes qui vient d'\\xc3\\xaatre assassin\\xc3\\xa9e au Sri Lanka est M. Kumar Ponnambalam, qui avait rendu visite au Parlement europ\\xc3\\xa9en il y a quelques mois \\xc3\\xa0 peine.\\n\"\n",
      " b\"Ne pensez-vous pas, Madame la Pr\\xc3\\xa9sidente, qu'il conviendrait d'\\xc3\\xa9crire une lettre au pr\\xc3\\xa9sident du Sri Lanka pour lui communiquer que le Parlement d\\xc3\\xa9plore les morts violentes, dont celle de M. Ponnambalam, et pour l'inviter instamment \\xc3\\xa0 faire tout ce qui est en son pouvoir pour chercher une r\\xc3\\xa9conciliation pacifique et mettre un terme \\xc3\\xa0 cette situation particuli\\xc3\\xa8rement difficile.\\n\"\n",
      " b\"En attendant, je souhaiterais, comme un certain nombre de coll\\xc3\\xa8gues me l'ont demand\\xc3\\xa9, que nous observions une minute de silence pour toutes les victimes, des temp\\xc3\\xaates notamment, dans les diff\\xc3\\xa9rents pays de l'Union europ\\xc3\\xa9enne qui ont \\xc3\\xa9t\\xc3\\xa9 touch\\xc3\\xa9s.\\n\"], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.\\n'\n",
      " b'I would like your advice about Rule 143 concerning inadmissibility.\\n'\n",
      " b'One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\\n'\n",
      " b\"Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\\n\"\n",
      " b\"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\n\"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "path_fr = glob.glob('./training/test_fr.txt')\n",
    "path_en = glob.glob('./training/test_en.txt') \n",
    "\n",
    "def load_data(path):\n",
    "    sentences = []\n",
    "    with open(path,'r',encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            sentences.append(line)\n",
    "\n",
    "    return np.array(sentences)\n",
    "\n",
    "source_raw = load_data(path_fr[0])\n",
    "target_raw = load_data(path_en[0])\n",
    "\n",
    "#print(type(source_raw))\n",
    "#print(len(source_raw)==len(target_raw))\n",
    "#print(source_raw[-1])\n",
    "#print(target_raw[-1])\n",
    "\n",
    "# Create a tf.data dataset\n",
    "BUFFER_SIZE = len(source_raw)\n",
    "BATCH_SIZE = 64 # 80\n",
    "\n",
    "is_train = np.random.uniform(size=(len(target_raw),)) < 0.8\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((source_raw[is_train], target_raw[is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "val_raw = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((source_raw[~is_train], target_raw[~is_train]))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE))\n",
    "\n",
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "  print(example_context_strings[:5])\n",
    "  print()\n",
    "  print(example_target_strings[:5])\n",
    "  break\n",
    "\n",
    "# TEXT PREPROCESSING\n",
    "\n",
    "# Standardization\n",
    "def tf_Standardization(text):\n",
    "  # Split accented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text\n",
    "\n",
    "# Vectorization : tf  layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens.\n",
    "\n",
    "MAX_VOCAB_SIZE = 3000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_Standardization,\n",
    "    max_tokens=MAX_VOCAB_SIZE,\n",
    "    ragged=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "054bff7a3e40494a77b69c2839a8742333280dcacbaf25f3affaa84064d3e215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
