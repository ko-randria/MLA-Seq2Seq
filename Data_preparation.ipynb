{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.layers import TextVectorization\n",
    "import glob\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes qui peuvent aider:\n",
    "- English-to-French translation\n",
    "- Bilingual, parallel corpora provided by ACL WMT  14\n",
    "\n",
    "reduce the size of the corpus using the selection method by Axelrod et al.\n",
    "\n",
    "\n",
    "Preprocesing of the data:\n",
    "- tokenization\n",
    "- use a shortlist of 30,000 most frequent words in each langage to train the model\n",
    "- any word not included in the shortlist is mapped to a special token ([UNK])\n",
    "- no other preprocessing method (no lowercasinf or stemming)\n",
    "\n",
    "- 2 types of data \n",
    "\n",
    "\n",
    "sentences of up to 30 words and sentences of up to 50 words\n",
    "\n",
    "- each update direction is computed using a minibatch of 80 sentences\n",
    "\n",
    "- once the model is trained, use a beam search to find a translation that approximately maximizes the conditional probability\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the database \"News Commentary\" database (parallel corpora provided by ACL WMT  14)\n",
    "#path_fr = glob.glob('./training/news-commentary-v9.fr-en.fr')\n",
    "#path_en = glob.glob('./training/news-commentary-v9.fr-en.en') \n",
    "\n",
    "\n",
    "# For the database \"Europarl\" database (parallel corpora provided by ACL WMT  14)\n",
    "path_fr = glob.glob('./training/europarl-v7.fr-en.fr')\n",
    "path_en = glob.glob('./training/europarl-v7.fr-en.en') \n",
    "\n",
    "print(path_fr)\n",
    "print(path_en)\n",
    "\n",
    "# Parameter\n",
    "max_features = 30000 # Max vocab size\n",
    "sentences = []\n",
    "\n",
    "dataset = tf.data.TextLineDataset(path_fr)\n",
    "print(dataset)\n",
    "\n",
    "#with open(path_fr[0],'r',encoding=\"utf8\") as file:\n",
    "#    for line in file:\n",
    "#        sentences.append(line)\n",
    "#test = tf.constant(sentences)\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(max_tokens= max_features)\n",
    "\n",
    "vectorize_layer.adapt(dataset)\n",
    "\n",
    "vectorize_layer.get_vocabulary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "054bff7a3e40494a77b69c2839a8742333280dcacbaf25f3affaa84064d3e215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
